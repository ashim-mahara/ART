# config.yaml
name: qwen3-moe-lora-sft

# Single node; single H200 on CoreWeave K8s
num_nodes: 1
resources:
  infra: k8s
  accelerators: H200:1          # uses the cluster's GPU label catalog
  cpus: 16+
  memory: 64+
  # Use a CUDA 12.4 + Torch 2.6 base with swift preinstalled; we will upgrade swift in setup.
  image_id: docker:modelscope-registry.us-west-1.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py310-torch2.6.0-vllm0.8.5.post1-modelscope1.27.1-swift3.5.3

envs:
  HF_HOME: /cache/hf
  TRANSFORMERS_CACHE: /cache/hf
  MODELSCOPE_CACHE: /cache/ms
  # Uncomment if you need tokenized access:
  # HF_TOKEN: <paste-if-needed>

# Everything below runs *inside* the container on the K8s node.
setup: |2
  set -euxo pipefail
  mkdir -p /cache/hf /cache/ms /workspace/data /workspace/output

  # Build deps for Apex; keep lean.
  apt-get update && apt-get install -y git build-essential ninja-build && rm -rf /var/lib/apt/lists/*

  # Upgrade ms-swift to get Megatron-SWIFT LoRA support; install deps per docs.
  python3 -m pip install --upgrade --no-cache-dir pip
  python3 -m pip install --no-cache-dir \
      "ms-swift>=3.8" transformers datasets accelerate pybind11

  # TransformerEngine (recommended for fused kernels / Flash attention path).
  python3 -m pip install --no-build-isolation "git+https://github.com/NVIDIA/TransformerEngine.git@release_v2.3"

  # Apex (commit pinned from ms-swift docs for CUDA 12.4 compatibility).
  git clone https://github.com/NVIDIA/apex /tmp/apex
  cd /tmp/apex
  git checkout e13873debc4699d39c6861074b9a3b2a02327f92
  python3 -m pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation \
      --config-settings "--build-option=--cpp_ext" \
      --config-settings "--build-option=--cuda_ext" .

  # Megatron-Core pinned version used by Megatron-SWIFT.
  python3 -m pip install --no-cache-dir "git+https://github.com/NVIDIA/Megatron-LM.git@core_r0.12.0"

  # Tiny local SFT dataset to avoid external fetches; valid ms-swift "messages" format.
  python3 - <<'PY'
  import json, os
  os.makedirs('/workspace/data', exist_ok=True)
  rows = [
    {"messages":[{"role":"system","content":"You are helpful."},{"role":"user","content":"Write a haiku about the ocean."},{"role":"assistant","content":"Waves count ancient time /\nSalt maps the moonlit harbor /\nGulls stitch dawn to foam."}]},
    {"messages":[{"role":"user","content":"Summarize: Transformers let tokens attend to each other."},{"role":"assistant","content":"Transformers use attention so tokens can select relevant context in parallel, enabling efficient, scalable sequence modeling."}]},
    {"messages":[{"role":"user","content":"Convert 37 °C to Fahrenheit."},{"role":"assistant","content":"37 °C is 98.6 °F."}]},
    {"messages":[{"role":"user","content":"List three prime numbers under 10."},{"role":"assistant","content":"2, 3, and 5 (and 7 is a bonus)."}]},
    {"messages":[{"role":"user","content":"Explain Mixture-of-Experts in one sentence."},{"role":"assistant","content":"MoE routes each token through a small subset of specialized feed-forward experts, increasing capacity without proportional compute."}]}
  ]
  with open('/workspace/data/minisft.jsonl','w') as f:
    for r in rows: f.write(json.dumps(r, ensure_ascii=False) + "\n")
  PY

run: |2
  set -euxo pipefail
  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

  # 1) Convert Hugging Face weights -> Megatron-Core format (Qwen3 MoE)
  #    Smallest open Qwen3 MoE: Qwen3-30B-A3B (≈3B active params).
  swift export \
    --model Qwen/Qwen3-30B-A3B-Instruct-2507 \
    --to_mcore true \
    --torch_dtype bfloat16 \
    --output_dir /workspace/qwen3-30b-a3b-mcore \
    --test_convert_precision true

  # 2) Single-GPU LoRA SFT with Megatron-SWIFT (memory-friendly knobs)
  #    LoRA + Flash attention + selective recompute on attention+MoE; SP on.
  megatron sft \
    --load /workspace/qwen3-30b-a3b-mcore \
    --dataset /workspace/data/minisft.jsonl \
    --train_type lora \
    --lora_rank 8 \
    --lora_alpha 16 \
    --target_modules all-linear \
    --attention_backend flash \
    --sequence_parallel true \
    --recompute_granularity selective \
    --recompute_modules core_attn moe \
    --micro_batch_size 2 \
    --global_batch_size 8 \
    --finetune true \
    --lr 1e-4 \
    --lr_warmup_fraction 0.05 \
    --min_lr 1e-5 \
    --max_epochs 1 \
    --save /workspace/output/qwen3-30b-a3b-lora \
    --save_interval 100000 \
    --max_length 1024 \
    --num_workers 2 \
    --no_save_optim true \
    --no_save_rng true \
    --log_throughput true

  # 3) Merge LoRA adapters -> HF format (for vLLM/Transformers/TGI)
  LAST=$(ls -d /workspace/output/qwen3-30b-a3b-lora/v* | sort | tail -n1)
  swift export \
    --mcore_adapters "$LAST" \
    --to_hf true \
    --torch_dtype bfloat16 \
    --output_dir /workspace/output/qwen3-30b-a3b-lora-hf \
    --test_convert_precision true

  # 4) Smoke test: load merged HF model and generate one answer
  python3 - <<'PY'
  from transformers import AutoTokenizer, AutoModelForCausalLM
  import torch, os
  mdir="/workspace/output/qwen3-30b-a3b-lora-hf"
  tok=AutoTokenizer.from_pretrained(mdir)
  model=AutoModelForCausalLM.from_pretrained(mdir, torch_dtype=torch.bfloat16, device_map="auto")
  prompt="User: In one sentence, what is LoRA?\nAssistant:"
  x=tok(prompt, return_tensors="pt").to(model.device)
  y=model.generate(**x, max_new_tokens=32, do_sample=False)
  print(tok.decode(y[0], skip_special_tokens=True))
  PY

config:
  kubernetes:
    provision_timeout: 900