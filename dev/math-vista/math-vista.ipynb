{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a6ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d51078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd70e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "splits = {\n",
    "    \"testmini\": \"data/testmini-00000-of-00001-725687bf7a18d64b.parquet\",\n",
    "    \"test\": \"data/test-*.parquet\",\n",
    "}\n",
    "df = pl.read_parquet(\"hf://datasets/AI4Math/MathVista/\" + splits[\"testmini\"]).sample(\n",
    "    fraction=1.0, shuffle=True, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e02b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, TypedDict, cast\n",
    "\n",
    "\n",
    "class DecodedImage(TypedDict):\n",
    "    bytes: bytes\n",
    "\n",
    "\n",
    "class Scenario(TypedDict):\n",
    "    pid: int\n",
    "    question: str\n",
    "    answer: str\n",
    "    image: str\n",
    "    decoded_image: DecodedImage\n",
    "\n",
    "\n",
    "val_scenarios = cast(list[Scenario], df.head(64).to_dicts())\n",
    "train_scenarios_iter = cast(Iterator[Scenario], df.tail(-64).iter_rows(named=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9287d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhilton\u001b[0m (\u001b[33mwandb\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sky/sky_workdir/dev/math-vista/wandb/run-20251014_221249-001</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/wandb/math-vista/runs/001' target=\"_blank\">001</a></strong> to <a href='https://wandb.ai/wandb/math-vista' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb/math-vista' target=\"_blank\">https://wandb.ai/wandb/math-vista</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb/math-vista/runs/001' target=\"_blank\">https://wandb.ai/wandb/math-vista/runs/001</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-14 22:12:56 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore # noqa: F401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-14 22:13:03 [__init__.py:235] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.6: Fast Qwen2_5_Vl patching. Transformers: 4.53.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.811 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "Unsloth: Vision model detected, setting approx_max_num_seqs to 1\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.66%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 139.81 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 1.\n",
      "Unsloth: vLLM's KV Cache can use up to 104.11 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 10-14 22:13:19 [config.py:1604] Using max model len 32768\n",
      "WARNING 10-14 22:13:19 [arg_utils.py:1511] --enable-prefix-caching is not supported for multimodal models in V0 and has been disabled.\n",
      "INFO 10-14 22:13:19 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=32768.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['embed_tokens', 'embedding', 'lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'router', 'visual', 'model.visual.blocks.31.mlp', 'model.visual.blocks.24.attn', 'model.visual.blocks.30.mlp', 'model.visual.blocks.30.attn', 'model.visual.blocks.25.attn', 'model.visual.blocks.29.attn', 'model.visual.blocks.26.attn', 'model.visual.blocks.28.attn', 'model.visual.blocks.19.attn', 'model.visual.blocks.29.mlp', 'model.visual.blocks.28.mlp', 'model.visual.blocks.31.attn', 'model.visual.blocks.25.mlp', 'model.visual.blocks.26.mlp', 'model.visual.blocks.20.attn', 'model.visual.blocks.27.mlp', 'model.visual.blocks.17.attn', 'model.visual.blocks.24.mlp', 'model.visual.blocks.18.attn', 'model.visual.blocks.16.attn', 'model.visual.blocks.11.attn', 'model.visual.blocks.21.mlp', 'model.visual.blocks.20.mlp', 'model.visual.blocks.23.mlp', 'model.visual.blocks.9.attn', 'model.visual.blocks.12.attn', 'model.visual.blocks.23.attn', 'model.visual.blocks.19.mlp', 'model.visual.blocks.22.mlp', 'model.visual.blocks.18.mlp', 'model.visual.blocks.13.attn', 'model.visual.blocks.8.attn', 'model.visual.blocks.11.mlp', 'model.visual.blocks.10.mlp', 'model.visual.blocks.6.attn', 'model.visual.blocks.15.mlp', 'model.visual.blocks.8.mlp', 'model.visual.blocks.9.mlp', 'model.visual.blocks.14.attn', 'model.visual.blocks.5.mlp', 'model.visual.blocks.14.mlp', 'model.visual.blocks.10.attn', 'model.visual.blocks.6.mlp', 'model.visual.blocks.7.mlp', 'model.visual.blocks.5.attn', 'model.visual.blocks.4.mlp', 'model.visual.blocks.16.mlp', 'model.visual.blocks.12.mlp', 'model.visual.blocks.13.mlp', 'model.visual.blocks.2.mlp', 'model.visual.blocks.3.mlp', 'model.visual.blocks.1.attn', 'model.visual.blocks.0.attn', 'model.visual.blocks.4.attn', 'model.visual.blocks.2.attn', 'model.visual.blocks.15.attn', 'model.visual.blocks.3.attn', 'model.visual.blocks.1.mlp', 'model.visual.blocks.17.mlp', 'model.visual.blocks.0.mlp', 'model.visual.blocks.7.attn', 'model.visual.blocks.31.mlp.down_proj'], 'llm_int8_threshold': 6.0}\n",
      "INFO 10-14 22:13:19 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":1,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 10-14 22:13:22 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 10-14 22:13:22 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-14 22:13:22 [model_runner.py:1083] Starting to load model unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit...\n",
      "WARNING 10-14 22:13:23 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 10-14 22:13:23 [bitsandbytes_loader.py:733] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 10-14 22:13:24 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 10-14 22:13:40 [weight_utils.py:312] Time spent downloading weights for unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit: 16.669816 seconds\n",
      "INFO 10-14 22:13:41 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 34.24it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.03it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-14 22:13:42 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 10-14 22:13:42 [model_runner.py:1115] Model loading took 6.5306 GiB and 19.065715 seconds\n",
      "INFO 10-14 22:13:54 [worker.py:295] Memory profiling takes 11.32 seconds\n",
      "INFO 10-14 22:13:54 [worker.py:295] the current vLLM instance can use total_gpu_memory (139.81GiB) x gpu_memory_utilization (0.79) = 109.98GiB\n",
      "INFO 10-14 22:13:54 [worker.py:295] model weights take 6.53GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 5.01GiB; the rest of the memory reserved for KV Cache is 98.27GiB.\n",
      "INFO 10-14 22:13:54 [executor_base.py:113] # cuda blocks: 115005, # CPU blocks: 7021\n",
      "INFO 10-14 22:13:54 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 56.15x\n",
      "INFO 10-14 22:13:57 [vllm_utils.py:721] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 10-14 22:13:57 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-14 22:14:00 [model_runner.py:1537] Graph capturing finished in 3 secs, took 0.06 GiB\n",
      "INFO 10-14 22:14:00 [vllm_utils.py:728] Unsloth: Patched vLLM v0 graph capture finished in 3 secs.\n",
      "INFO 10-14 22:14:00 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 17.86 seconds\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'input_layernorm', 'post_layernorm', 'norm1', 'q_norm', 'post_feedforward_layernorm', 'layer_norm2', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'k_norm', 'norm2']\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'input_layernorm', 'post_layernorm', 'q_norm', 'post_feedforward_layernorm', 'cross_attn_input_layernorm', 'layer_norm2', 'pre_feedforward_layernorm', 'post_attention_layernorm', 'k_norm', 'cross_attn_post_attention_layernorm']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import art\n",
    "from art.local import LocalBackend\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"001\",\n",
    "    project=\"math-vista\",\n",
    "    base_model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    ")\n",
    "backend = LocalBackend()\n",
    "await model.register(backend)\n",
    "client = model.openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92b4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rollout(scenario: Scenario) -> art.Trajectory:\n",
    "    image_path = f\"/tmp/{scenario['image']}\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(scenario[\"decoded_image\"][\"bytes\"])\n",
    "\n",
    "    trajectory = art.Trajectory(messages_and_choices=[], reward=0.0)\n",
    "    trajectory.messages_and_choices = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": scenario[\"question\"]\n",
    "                    + \"\\n\\nNote: Provide your answer in a LaTeX box.\",\n",
    "                },\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"file://{image_path}\"}},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        model=model.name, messages=trajectory.messages()\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    trajectory.messages_and_choices.append(choice)\n",
    "    content = choice.message.content\n",
    "    assert content is not None\n",
    "    if matches := list(re.finditer(r\"\\\\boxed\\{(.*?)\\}\", content, re.DOTALL)):\n",
    "        match = matches[-1]\n",
    "        answer = match.group(1)\n",
    "        if answer.lower() == scenario[\"answer\"].lower():\n",
    "            trajectory.reward = 1.0\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359e530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e87359e46a44160953c99e33f5b3843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather(val):   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f242f9d165145f6b55dfe2913d565de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather(train):   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "\n",
    "SCENARIOS_PER_STEP = 8\n",
    "TRAJECTORY_GROUP_SIZE = 8\n",
    "start = await model.get_step()\n",
    "train_scenarios_iter = itertools.cycle(train_scenarios_iter)\n",
    "for _ in range(start * SCENARIOS_PER_STEP):\n",
    "    next(train_scenarios_iter)\n",
    "\n",
    "for i in range(start, 1000):\n",
    "    train_scenarios = [next(train_scenarios_iter) for _ in range(SCENARIOS_PER_STEP)]\n",
    "    val_trajectories, train_trajectory_groups = await asyncio.gather(\n",
    "        art.gather_trajectories(\n",
    "            (rollout(scenario) for scenario in val_scenarios),\n",
    "            pbar_desc=\"gather(val)\",\n",
    "            max_exceptions=32,\n",
    "        ),\n",
    "        art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(\n",
    "                    rollout(scenario) for _ in range(TRAJECTORY_GROUP_SIZE)\n",
    "                )\n",
    "                for scenario in train_scenarios\n",
    "            ),\n",
    "            pbar_desc=\"gather(train)\",\n",
    "            max_exceptions=32,\n",
    "        ),\n",
    "    )\n",
    "    await model.log(val_trajectories)\n",
    "    break\n",
    "    await model.train(train_trajectory_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9374ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in (\n",
    "    train_trajectory_groups[0].trajectories[0].messages_and_choices[0][\"content\"]\n",
    "):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6efec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "await model.train(train_trajectory_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.transformers.patches import patch_preprocess_mask_arguments\n",
    "\n",
    "patch_preprocess_mask_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trajectories = await art.gather_trajectories(\n",
    "    (rollout(scenario) for scenario in val_scenarios)\n",
    ")\n",
    "await model.log(val_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trajectories[0].messages_and_choices[-1].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee774cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "    re.finditer(\n",
    "        r\"\\\\boxed\\{(.*?)\\}\",\n",
    "        val_trajectories[0].messages_and_choices[-1].message.content,\n",
    "        re.DOTALL,\n",
    "    )\n",
    ")[-1].group(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
